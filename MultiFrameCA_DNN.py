#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon May 19 16:30:54 2025

@author: fran
"""
import os
import numpy as np
import tensorflow as tf
import pickle

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout, TimeDistributed, Conv2D, MaxPooling2D, Input
from tensorflow.keras.optimizers.legacy import Adam
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from tensorflow.compat.v1 import ConfigProto, InteractiveSession

# Import functions from the utils folder
from utils.plotting_utils import plot_single_history

import h5py

# Configure GPU memory growth
config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

# --- Define Parameters ---
DATA_DIR = 'labeled_sequences'  # Root directory containing train/test subdirectories
SEQUENCE_LENGTH = 10  # Must match the SEQUENCE_LENGTH used during data generation
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 16
EPOCHS = 30
LEARNING_RATE = 1e-4
OUTPUT_DIR = 'models_multiframe'
MODEL_NAME = f'CNN_Multiframe_SeqLen_{SEQUENCE_LENGTH}'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Load and Preprocess Data ---
def load_labeled_npy_sequences(data_dir, sequence_length, img_height, img_width):
    sequences = []
    labels = []
    class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
    for label_idx, class_name in enumerate(class_names):
        class_path = os.path.join(data_dir, class_name)
        for seq_file in os.listdir(class_path):
            if seq_file.endswith('.npy'):
                try:
                    sequence = np.load(os.path.join(class_path, seq_file))
                    label = int(seq_file.split('_label_')[-1].replace('.npy', ''))
                    if sequence.shape == (sequence_length, img_height, img_width, 3):
                        sequences.append(sequence)
                        labels.append(label)
                    else:
                        print(f"Warning: Skipping file with incorrect shape: {seq_file} in {class_path}")
                except Exception as e:
                    print(f"Error loading or processing file {seq_file} in {class_path}: {e}")
    return np.array(sequences), np.array(labels), class_names

def load_labeled_hdf5_sequences(data_dir, sequence_length, img_height, img_width):
    """
    Loads labeled video sequences from HDF5 files generated by
    create_labeled_sequences_from_annotations.

    Args:
        data_dir (str): Directory containing the 'train' and 'test' subdirectories.
        sequence_length (int): The expected length of each sequence.
        img_height (int): The expected height of each frame.
        img_width (int): The expected width of each frame.

    Returns:
        tuple: (train_sequences, test_sequences, train_labels, test_labels, class_names)
            - train_sequences: NumPy array of shape (num_train_sequences, sequence_length, img_height, img_width, 3).
            - test_sequences: NumPy array of shape (num_test_sequences, sequence_length, img_height, img_width, 3).
            - train_labels: NumPy array of shape (num_train_sequences,).
            - test_labels: NumPy array of shape (num_test_sequences,).
            - class_names: List of class names (e.g., ['no_collision', 'collision']).
    """

    train_sequences, train_labels = [], []
    test_sequences, test_labels = [], []
    class_names = ['no_collision', 'collision']  # Fixed based on labels (0, 1)

    for split_dir in ['train', 'test']:
        split_path = os.path.join(data_dir, split_dir)
        if not os.path.isdir(split_path):
            print(f"Warning: {split_path} not found or not a directory")
            continue

        for video_dir in os.listdir(split_path):
            video_path = os.path.join(split_path, video_dir)
            if os.path.isdir(video_path):
                h5_path = os.path.join(video_path, f"data_seq{sequence_length}.hdf5")
            else:
                continue  # Skip if not a directory

            if os.path.exists(h5_path):
                print(f"Loading data from {h5_path}")
                try:
                    with h5py.File(h5_path, 'r') as hf:
                        loaded_sequences = hf['sequences'][:]
                        loaded_labels = hf['labels'][:]

                    # Check the shape of the loaded sequences
                    if loaded_sequences.shape[1:] != (sequence_length, img_height, img_width, 3):
                        print(
                            f"Warning: Skipping data from {h5_path} due to incorrect sequence shape. Expected "
                            f"({sequence_length}, {img_height}, {img_width}, 3), got {loaded_sequences.shape[1:]}"
                        )
                        continue

                    if split_dir == 'train':
                        train_sequences.extend(loaded_sequences)
                        train_labels.extend(loaded_labels)
                    else:  # test
                        test_sequences.extend(loaded_sequences)
                        test_labels.extend(loaded_labels)

                except Exception as e:
                    print(f"Error loading data from {h5_path}: {e}")
            else:
                print(f"Warning: HDF5 file not found: {h5_path}")

    return (
        np.array(train_sequences) if train_sequences else np.array([]),
        np.array(test_sequences) if test_sequences else np.array([]),
        np.array(train_labels) if train_labels else np.array([]),
        np.array(test_labels) if test_labels else np.array([]),
        class_names
    )

# Usage
print("Loading data sequences...")
train_sequences, test_sequences, train_labels, test_labels, class_names = load_labeled_hdf5_sequences(
    DATA_DIR, SEQUENCE_LENGTH, IMG_HEIGHT, IMG_WIDTH
)

# --- Split Training Data for Validation ---
print("Split Training Data for Validation...")
train_sequences, val_sequences, train_labels, val_labels = train_test_split(
    train_sequences, train_labels, test_size=0.2, stratify=train_labels, random_state=42
)

# Standardize to zero mean, unit variance
mean = np.mean(train_sequences, axis=(0, 1, 2, 3))  # Average over sequences, frames, height, width
std = np.std(train_sequences, axis=(0, 1, 2, 3))    # Average over sequences, frames, height, width

# Apply standardization
train_sequences = (train_sequences - mean) / std
val_sequences = (val_sequences - mean) / std
test_sequences = (test_sequences - mean) / std

# --- Calculate Class Weights ---
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
class_weight_dict = dict(enumerate(class_weights))
print("Class Weights:", class_weight_dict)

# --- Build Multi-Frame CNN Model ---
print("Build Multi-Frame CNN Model...")
def build_multiframe_cnn(sequence_length, img_height, img_width, num_classes):
    model = Sequential([
        Input(shape=(sequence_length, img_height, img_width, 3)),  # Define input shape here
        TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same')),
        TimeDistributed(MaxPooling2D((2, 2))),
        TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),
        TimeDistributed(MaxPooling2D((2, 2))),
        TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')),
        TimeDistributed(MaxPooling2D((2, 2))),
        TimeDistributed(Flatten()),
        tf.keras.layers.LSTM(64, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    return model

num_classes = len(class_names)
model = build_multiframe_cnn(SEQUENCE_LENGTH, IMG_HEIGHT, IMG_WIDTH, num_classes)
model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# --- Train the Model ---
history = model.fit(
    train_sequences, train_labels,
    validation_data=(val_sequences, val_labels),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    class_weight=class_weight_dict
)

# --- Evaluate the Model ---
loss, accuracy = model.evaluate(test_sequences, test_labels, batch_size=BATCH_SIZE)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# --- Plot Training History ---
plot_single_history(history, save_path=os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_history.png'))

# --- Save the Model and Class Names ---
model.save(os.path.join(OUTPUT_DIR, f'{MODEL_NAME}.keras'))
with open(os.path.join(OUTPUT_DIR, f'{MODEL_NAME}_classes.pkl'), 'wb') as f:
    pickle.dump(class_names, f)

session.close()